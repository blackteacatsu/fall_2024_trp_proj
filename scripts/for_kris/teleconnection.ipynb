{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this under the pytorch (machine learning) conda environment.\n",
    "\n",
    "```powershell\n",
    "conda activate pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import rasterio\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import hypergeom\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import os\n",
    "from numba import jit, cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries = ['Mexico', 'Indonesia', 'Brazil', 'Thailand', 'South Africa', 'India', 'Colombia', 'Peru', 'Philippines', 'Egypt', 'Kenya', 'Chile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_la_nina_sequences(df):\n",
    "    \"\"\"\n",
    "    Labels La Nina sequences where the maximum sequence number is greater than 5.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing La_Nina_Seq column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original data with additional label column for significant sequences\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Initialize the label column\n",
    "    df_copy['sig_elnino'] = 0\n",
    "    df_copy['sig_lanina'] = 0\n",
    "    \n",
    "    # Function to find max in a sequence\n",
    "    def get_sequence_max(start_idx, seq_in):\n",
    "        sequence = []\n",
    "        idx = start_idx\n",
    "        while idx < len(df_copy) and df_copy[seq_in].iloc[idx] != 0:\n",
    "            sequence.append(df_copy[seq_in].iloc[idx])\n",
    "            idx += 1\n",
    "        return max(sequence) if sequence else 0\n",
    "\n",
    "    # Iterate through the dataframe\n",
    "    i = 0\n",
    "    seq = 'La_Nina_Seq'\n",
    "    while i < len(df_copy):\n",
    "        if df_copy[seq].iloc[i] == 1:  # Start of a sequence\n",
    "            max_in_sequence = get_sequence_max(i, seq)\n",
    "            if max_in_sequence > 5:\n",
    "                # Label all numbers in this sequence\n",
    "                j = i\n",
    "                while j < len(df_copy) and df_copy[seq].iloc[j] != 0:\n",
    "                    df_copy.loc[df_copy.index[j], 'sig_lanina'] = 1\n",
    "                    j += 1\n",
    "            # Skip to end of current sequence\n",
    "            while i < len(df_copy) and df_copy[seq].iloc[i] != 0:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "     # Iterate through the dataframe\n",
    "    i = 0\n",
    "    seq = 'El_Nino_Seq'\n",
    "    while i < len(df_copy):\n",
    "        if df_copy[seq].iloc[i] == 1:  # Start of a sequence\n",
    "            max_in_sequence = get_sequence_max(i, seq)\n",
    "            if max_in_sequence > 5:\n",
    "                # Label all numbers in this sequence\n",
    "                j = i\n",
    "                while j < len(df_copy) and df_copy[seq].iloc[j] != 0:\n",
    "                    df_copy.loc[df_copy.index[j], 'sig_elnino'] = 1\n",
    "                    j += 1\n",
    "            # Skip to end of current sequence\n",
    "            while i < len(df_copy) and df_copy[seq].iloc[i] != 0:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def calculate_terciles(data):\n",
    "    \n",
    "    # Compute climatological terciles for each month\n",
    "    tercile_bounds = data.groupby(\"month\")[\"values\"].quantile([0.33, 0.67]).unstack()\n",
    "    tercile_bounds.columns = ['lower_33', 'upper_67']\n",
    "\n",
    "    # Merge tercile bounds with full dataset\n",
    "    data = data.merge(tercile_bounds, on=\"month\", how=\"left\")\n",
    "\n",
    "    # Assign tercile categories\n",
    "    conditions = [\n",
    "        data[\"values\"] < data[\"lower_33\"],  # Below-normal (-1)\n",
    "        (data[\"values\"] >= data[\"lower_33\"]) & (data[\"values\"] <= data[\"upper_67\"]),  # Near-normal (0)\n",
    "        data[\"values\"] > data[\"upper_67\"]   # Above-normal (1)\n",
    "    ]\n",
    "    choices = [-1, 0, 1]\n",
    "    data[\"tercile\"] = np.select(conditions, choices)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def hypergeometric_significance(n, b, r, x):\n",
    "    \"\"\"\n",
    "    Computes the statistical significance of above- or below-normal signals using the hypergeometric test.\n",
    "\n",
    "    Parameters:\n",
    "    - n: Total number of months with sufficient data\n",
    "    - b: Total number of months classified as above- or below-normal\n",
    "    - r: Number of El Niño or La Niña months (sample size)\n",
    "    - x: Observed number of above- or below-normal months in ENSO years\n",
    "\n",
    "    Returns:\n",
    "    - p_value: Probability of selecting x or more extreme months by random chance\n",
    "    \"\"\"\n",
    "    # Compute right-tail probability (1 - CDF up to x-1)\n",
    "    p_value = stats.hypergeom.sf(x-1, n, b, r)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names manually\n",
    "column_monthly = [\"Year\", \"Month\", \"NINO1+2\", \"ANOM_NINO1+2\", \"NINO3\", \"ANOM_NINO3\",\n",
    "                \"NINO4\", \"ANOM_NINO4\", \"NINO3.4\", \"ANOM_NINO3.4\"]\n",
    "column_seasonal = ['SEAS', 'YR', 'TOTAL', 'ANOM']\n",
    "\n",
    "# Read the file\n",
    "# https://www.cpc.ncep.noaa.gov/data/indices/ersst5.nino.mth.91-20.ascii\n",
    "monthly = pd.read_csv(\"data/ersst5_nino_monthly.txt\", sep='\\s+', names=column_monthly, skiprows=1)[['Year', 'Month', 'NINO3.4', 'ANOM_NINO3.4']]\n",
    "seasonal = pd.read_csv(\"data/oni_seasonal.txt\", sep='\\s+', names=column_seasonal, skiprows=1)\n",
    "\n",
    "# Identify El Niño and La Niña periods\n",
    "seasonal['El_Nino'] = (seasonal['ANOM'] >= 0.5).astype(int)\n",
    "seasonal['La_Nina'] = (seasonal['ANOM'] <= -0.5).astype(int)\n",
    "\n",
    "# Compute running count of consecutive months where ONI exceeds 0.5 or is below -0.5\n",
    "seasonal['El_Nino_Seq'] = seasonal['El_Nino'] * seasonal['El_Nino'].groupby((seasonal['El_Nino'] != seasonal['El_Nino'].shift()).cumsum()).transform('cumsum')\n",
    "seasonal['La_Nina_Seq'] = seasonal['La_Nina'] * seasonal['La_Nina'].groupby((seasonal['La_Nina'] != seasonal['La_Nina'].shift()).cumsum()).transform('cumsum')\n",
    "\n",
    "enso_base = label_la_nina_sequences(seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_var = 'ssr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if climate_var == 'net-thermal':\n",
    "    # Load NetCDF file\n",
    "    file_path = \"data/net-thermal.nc\"\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    ds_values = ds['t2m'].values\n",
    "\n",
    "    lats = ds['latitude'].values\n",
    "    lons = ds['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "elif climate_var == \"snowcover\":\n",
    "    # Load NetCDF file\n",
    "    file_path = \"data/net-thermal.nc\"\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    ds_values = ds[''].values\n",
    "\n",
    "    lats = ds['latitude'].values\n",
    "    lons = ds['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "\"\"\"\n",
    "elif climate_var == 'precip':\n",
    "    file_path = \"data/precip.nc\"\n",
    "    ds1 = xr.open_dataset(file_path)\n",
    "\n",
    "    file_path = \"data/precip1.nc\"\n",
    "    ds2 = xr.open_dataset(file_path)\n",
    "    # Concatenate along the time dimension\n",
    "\n",
    "    ds_values = np.concatenate([ds1['tp'].values[:624], ds2['tp'].values, ds1['tp'].values[624:]], axis=0)\n",
    "\n",
    "    lats = ds1['latitude'].values\n",
    "    lons = ds1['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds1.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "elif climate_var == 'ssr': # solar radiation\n",
    "    file_path = \"data/ssr.nc\"\n",
    "    ds1 = xr.open_dataset(file_path)\n",
    "\n",
    "    file_path = \"data/ssr1.nc\"\n",
    "    ds2 = xr.open_dataset(file_path)\n",
    "    # Concatenate along the time dimension\n",
    "\n",
    "    ds_values = np.concatenate([ds1['ssr'].values[:624], ds2['ssr'].values, ds1['ssr'].values[624:]], axis=0)\n",
    "\n",
    "    lats = ds1['latitude'].values\n",
    "    lons = ds1['longitude'].values\n",
    "    lons = np.where(lons > 180, lons - 360, lons) # same as SPEI and crop\n",
    "    ds = ds1.assign_coords(longitude=lons)\n",
    "\n",
    "    # Create meshgrid of lat/lon pairs\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    coordinates = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:54<00:00, 24.52s/it]\n"
     ]
    }
   ],
   "source": [
    "world = gpd.read_file('data/ne_10m_admin_0_countries/ne_10m_admin_0_countries.shp')\n",
    "\n",
    "mask_dict = {}\n",
    "points_in_countries_all = {}\n",
    "points_idx_climate = {}\n",
    "\n",
    "for country_name in tqdm(countries):\n",
    "    country = world[world['SOVEREIGNT'] == country_name]\n",
    "    # Create a GeoDataFrame with the mesh grid points\n",
    "    points_gdf = gpd.GeoDataFrame(pd.DataFrame(coordinates, columns=['Longitude', 'Latitude']),\n",
    "                                geometry=gpd.points_from_xy(coordinates[:, 0], coordinates[:, 1]), crs=\"EPSG:4326\" )\n",
    "    \n",
    "    points_in_countries =  gpd.sjoin(points_gdf, country, predicate='within')\n",
    "    points_in_countries_all[country_name] = points_in_countries\n",
    "    mask = np.zeros(lon_grid.shape, dtype=bool)\n",
    "\n",
    "    indices = points_in_countries.index.values\n",
    "    mask[np.unravel_index(indices, lon_grid.shape)] = True\n",
    "    mask_dict[country_name] = mask\n",
    "    \n",
    "    index = []\n",
    "    for lon, lat in points_in_countries[['Longitude', 'Latitude']].values:\n",
    "        # Find the index of the closest point\n",
    "        lat_idx = abs(ds['latitude'] - lat).argmin().item()\n",
    "        lon_idx = abs(ds['longitude'] - lon).argmin().item()\n",
    "        index.append([lat_idx, lon_idx])\n",
    "    points_idx_climate[country_name] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to a pickle file\n",
    "# with open(\"outcome/lon_lat_idx_era5_12c.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(points_idx_climate, file)\n",
    "\n",
    "with open(\"outcome/lon_lat_idx_era5_12c.pkl\", \"rb\") as file:\n",
    "    points_idx_climate = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lats = range(1801)\n",
    "# lons = range(3600)\n",
    "# # Create meshgrid of lat/lon pairs\n",
    "# lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "# # Flatten the grids and create coordinate pairs\n",
    "# coordinates_idx = list(zip(lat_grid.ravel(), lon_grid.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_order = [\"DJF\", \"JFM\", \"FMA\", \"MAM\", \"AMJ\", \"MJJ\", \"JJA\", \"JAS\", \"ASO\", \"SON\", \"OND\", \"NDJ\"]\n",
    "states = ['El Nino', 'La Nina', 'Normal']\n",
    "tercile_order = ['BN', 'NN', 'AN']\n",
    "states_lower = ['elnino', 'lanina', 'normal']\n",
    "\n",
    "seasons_all = enso_base['SEAS'].values\n",
    "sig_elnino = enso_base['sig_elnino'].values\n",
    "sig_lanina = enso_base['sig_lanina'].values\n",
    "enso_masks = [\n",
    "        sig_elnino == 1,  # El Nino\n",
    "        sig_lanina == 1,  # La Nina\n",
    "        (sig_elnino == 0) & (sig_lanina == 0)  # Neutral\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17428/17428 [01:31<00:00, 189.72it/s]\n",
      "100%|██████████| 15287/15287 [01:17<00:00, 197.35it/s]\n",
      "100%|██████████| 70741/70741 [08:00<00:00, 147.19it/s]\n",
      "100%|██████████| 4338/4338 [00:29<00:00, 145.42it/s]\n",
      "100%|██████████| 11319/11319 [01:56<00:00, 97.51it/s]\n",
      "100%|██████████| 27853/27853 [03:29<00:00, 132.83it/s]\n",
      "100%|██████████| 9230/9230 [01:06<00:00, 138.02it/s]\n",
      "100%|██████████| 10635/10635 [01:17<00:00, 137.16it/s]\n",
      "100%|██████████| 2438/2438 [00:23<00:00, 104.72it/s]\n",
      "100%|██████████| 9151/9151 [01:10<00:00, 130.39it/s]\n",
      "100%|██████████| 4760/4760 [00:36<00:00, 128.71it/s]\n",
      "100%|██████████| 7694/7694 [01:05<00:00, 116.92it/s]\n"
     ]
    }
   ],
   "source": [
    "freq_all_country = {} # \n",
    "significance_all_country = {} # initialization \n",
    "\n",
    "for country in countries: \n",
    "    freq_all = []\n",
    "    significance_all = []\n",
    "    for lat, lon in tqdm(points_idx_climate[country]):\n",
    "        values = ds_values[:, lat, lon]\n",
    "        \n",
    "        if np.all(np.isnan(values)):\n",
    "            freq_all.append(np.full((3, len(season_order), 3), np.nan))\n",
    "            significance_all.append(np.full((3, 2), np.nan))\n",
    "            continue\n",
    "        \n",
    "        values_mon = np.reshape(values, (75, 12)).T # 75 years 12 months\n",
    "        tercile = np.nanquantile(values_mon, [0.33, 0.67], axis=1)\n",
    "        tercile_binary = np.select(\n",
    "                            [values_mon.T <= tercile[0], \n",
    "                            values_mon.T > tercile[1]],\n",
    "                            [-1, 1],\n",
    "                            default=0\n",
    "                        ).flatten()[:-1]\n",
    "\n",
    "        frequencies = np.zeros((3, len(season_order), 3))\n",
    "\n",
    "\n",
    "        for state_idx, state_mask in enumerate(enso_masks):\n",
    "            for seas_idx, season in enumerate(season_order):\n",
    "                season_mask = seasons_all == season\n",
    "                combined_mask = state_mask & season_mask\n",
    "                \n",
    "                if np.any(combined_mask):\n",
    "                    for tercile in [-1, 0, 1]:\n",
    "                        tercile_count = np.sum(((tercile_binary == tercile) & combined_mask))\n",
    "                        frequencies[state_idx, seas_idx, tercile + 1] = tercile_count / np.sum(combined_mask)\n",
    "\n",
    "        # Total number of months\n",
    "        total_months = len(tercile_binary)\n",
    "\n",
    "        # Create temperature tercile masks\n",
    "        above_mask = tercile_binary == 1\n",
    "        below_mask = tercile_binary == -1\n",
    "\n",
    "        # Count total above/below normal months\n",
    "        total_above = np.sum(above_mask)\n",
    "        total_below = np.sum(below_mask)\n",
    "\n",
    "        # Initialize arrays for results\n",
    "        enso_months = np.zeros(3)\n",
    "        observed_above = np.zeros(3)\n",
    "        observed_below = np.zeros(3)\n",
    "        p_values = np.zeros((3, 2))  # [above, below] for each ENSO state\n",
    "\n",
    "        # Calculate counts for each ENSO state\n",
    "        for i, mask in enumerate(enso_masks):\n",
    "            enso_months[i] = np.sum(mask)\n",
    "            observed_above[i] = np.sum(mask & above_mask)\n",
    "            observed_below[i] = np.sum(mask & below_mask)\n",
    "            \n",
    "            # Calculate p-values using hypergeometric distribution\n",
    "            # For above normal\n",
    "            p_values[i, 0] = hypergeom.sf(\n",
    "                observed_above[i] - 1,           # k-1 (observed successes minus 1)\n",
    "                total_months,                    # N (total population)\n",
    "                total_above,                     # K (total successes in population)\n",
    "                enso_months[i]                   # n (sample size)\n",
    "            )\n",
    "            \n",
    "            # For below normal\n",
    "            p_values[i, 1] = hypergeom.sf(\n",
    "                observed_below[i] - 1,           # k-1 (observed successes minus 1)\n",
    "                total_months,                    # N (total population)\n",
    "                total_below,                     # K (total successes in population)\n",
    "                enso_months[i]                   # n (sample size)\n",
    "            )\n",
    "\n",
    "        freq_all.append(frequencies)\n",
    "        significance_all.append(p_values)\n",
    "    freq_all_country[country] = freq_all\n",
    "    significance_all_country[country] = significance_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a pickle file\n",
    "with open(f\"outcome/frequency_12c_{climate_var}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(freq_all_country, file)\n",
    "\n",
    "with open(f\"outcome/significance_12c_{climate_var}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(significance_all_country, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the frequency data\n",
    "with open(f\"outcome/frequency_12c_{climate_var}.pkl\", \"rb\") as file:\n",
    "    freq_all_country = pickle.load(file)\n",
    "\n",
    "# Load the significance data\n",
    "with open(f\"outcome/significance_12c_{climate_var}.pkl\", \"rb\") as file:\n",
    "    significance_all_country = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_colormaps():\n",
    "    \"\"\"\n",
    "    Create two custom colormaps for Below Normal and Above Normal based on the image\n",
    "    \"\"\"\n",
    "    # Below Normal colors (yellow to brown)\n",
    "    below_colors = ['#f9fa04', '#e7b834', '#ce8033', '#a9451d', '#783100']\n",
    "    \n",
    "    # Above Normal colors (light green to blue)\n",
    "    above_colors = ['#d1f8cb', '#adf79f', '#75ba6f', '#4394cc', '#0c3af3']\n",
    "    \n",
    "    below_cmap = ListedColormap(below_colors)\n",
    "    above_cmap = ListedColormap(above_colors)\n",
    "    \n",
    "    return below_cmap, above_cmap\n",
    "\n",
    "def create_probability_maps(data_all, index_all, output_dir=f'outcome/map/{climate_var}'):\n",
    "    \"\"\"\n",
    "    Create maps showing the most likely category for each season\n",
    "    \n",
    "    Parameters:\n",
    "    - data: np.array with shape (17428, 3, 12, 3)\n",
    "    - idx_locations: list of tuples containing (lat_idx, lon_idx) for each coordinate\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create empty base map\n",
    "    base_map = np.full((1801, 3600), np.nan)\n",
    "    \n",
    "    # Get custom colormaps\n",
    "    below_cmap, above_cmap = create_custom_colormaps()\n",
    "    \n",
    "    # Probability thresholds from the image\n",
    "    prob_thresholds = [40, 45, 50, 60, 70]\n",
    "\n",
    "    # Create a new projection\n",
    "    projection = ccrs.PlateCarree(central_longitude=180)\n",
    "    data_transform = ccrs.PlateCarree()\n",
    "    \n",
    "    # Loop through all seasons and states\n",
    "    for season in range(12):\n",
    "        for state in range(3):\n",
    "            # Create a new map for this season and state\n",
    "            category_map = base_map.copy()\n",
    "            probability_map = base_map.copy()\n",
    "            \n",
    "            # For each location, find the most likely category and its probability\n",
    "            for country in countries:\n",
    "                idx_locations = np.array(index_all[country])\n",
    "                data = np.array(data_all[country])\n",
    "                for coord_idx, (lat_idx, lon_idx) in enumerate(idx_locations):\n",
    "                    # Get probabilities for all states in this season\n",
    "                    probs = data[coord_idx, state , season, :]\n",
    "                    \n",
    "                    # Find the highest probability and corresponding state\n",
    "                    max_prob = np.max(probs)\n",
    "                    max_state = np.argmax(probs)\n",
    "                    \n",
    "                    category_map[lat_idx, lon_idx] = max_state\n",
    "                    probability_map[lat_idx, lon_idx] = max_prob * 100  # Convert to percentage\n",
    "            \n",
    "            # Create the plot with cartopy projection\n",
    "            fig = plt.figure(figsize=(15, 8))\n",
    "            ax = plt.axes(projection=projection)\n",
    "\n",
    "            # Set the main map title\n",
    "            ax.set_title(f'State {states[state]}, Season {season_order[season]}', \n",
    "                        fontsize=14,    # Adjust font size\n",
    "                        pad=15,         # Adjust spacing between title and plot\n",
    "                        fontweight='bold')\n",
    "            \n",
    "            # Add coastlines and country borders\n",
    "            ax.coastlines(linewidth=0.5)\n",
    "            ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "            \n",
    "            # Add gridlines\n",
    "            gl = ax.gridlines(draw_labels=True, linewidth=0.2, color='gray', alpha=0.5)\n",
    "            gl.top_labels = False\n",
    "            gl.right_labels = False\n",
    "            \n",
    "            # Create masked arrays for below and above normal\n",
    "            below_mask = category_map == 0\n",
    "            above_mask = category_map == 2\n",
    "            \n",
    "            # Plot Below Normal probabilities\n",
    "            below_probs = np.ma.masked_where(~below_mask, probability_map)\n",
    "            ax.imshow(below_probs, \n",
    "                     transform=data_transform,\n",
    "                     extent=[0, 360, -90, 90],\n",
    "                     cmap=below_cmap, \n",
    "                     vmin=40, \n",
    "                     vmax=70)\n",
    "            \n",
    "            # Plot Above Normal probabilities\n",
    "            above_probs = np.ma.masked_where(~above_mask, probability_map)\n",
    "            ax.imshow(above_probs, \n",
    "                     transform=data_transform,\n",
    "                     extent=[0, 360, -90, 90],\n",
    "                     cmap=above_cmap, \n",
    "                     vmin=40, \n",
    "                     vmax=70)\n",
    "            \n",
    "\n",
    "            norm = plt.Normalize(40, 70)\n",
    "            \n",
    "            # Create ScalarMappables for both colormaps\n",
    "            sm_below = plt.cm.ScalarMappable(cmap=below_cmap, norm=norm)\n",
    "            sm_above = plt.cm.ScalarMappable(cmap=above_cmap, norm=norm)\n",
    "            \n",
    "            # Position the colorbars\n",
    "            # Below Normal colorbar\n",
    "            cbar_below_ax = fig.add_axes([0.125, 0.05, 0.35, 0.02])  # [left, bottom, width, height]\n",
    "            cbar_below = fig.colorbar(sm_below, cax=cbar_below_ax, orientation='horizontal')\n",
    "            cbar_below.set_ticks([46, 52, 58, 64, 70])\n",
    "            cbar_below.set_ticklabels(['40', '45', '50', '60', '70+'])\n",
    "            cbar_below.ax.set_title('Below Normal', pad=5)\n",
    "            \n",
    "            # Above Normal colorbar\n",
    "            cbar_above_ax = fig.add_axes([0.525, 0.05, 0.35, 0.02])  # [left, bottom, width, height]\n",
    "            cbar_above = fig.colorbar(sm_above, cax=cbar_above_ax, orientation='horizontal')\n",
    "            cbar_above.set_ticks([46, 52, 58, 64, 70])\n",
    "            cbar_above.set_ticklabels(['40', '45', '50', '60', '70+'])\n",
    "            cbar_above.ax.set_title('Above Normal', pad=5)\n",
    "            \n",
    "            # Add main title above both colorbars\n",
    "            fig.text(0.5, 0.15, 'Probability (%) of Most Likely Category', \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "            \n",
    "            # Adjust layout to make room for colorbars\n",
    "            plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "            # plt.show()\n",
    "            \n",
    "            # Add title\n",
    "            # plt.title(f'State {states[state]}, Season {season_order[season]}, Tercile {tercile_order[tercile]}')\n",
    "            # plt.title(f'State {states[state]}, Season {season_order[season]}')\n",
    "            # Save the plot\n",
    "            filename = f'prob_{states_lower[state]}_season_{season_order[season]}.png'\n",
    "            plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "def create_dual_p_value_maps(data_all, index_all, output_dir=f'outcome/map/{climate_var}'):\n",
    "    \"\"\"\n",
    "    Create side-by-side maps showing p-values for above (blue) and below (red)\n",
    "    \"\"\"\n",
    "    # Create colormaps\n",
    "    # Blue scheme for above\n",
    "    colors_above = ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#084594'][::-1]\n",
    "    cmap_above = LinearSegmentedColormap.from_list('custom_blues', colors_above)\n",
    "    \n",
    "    # Red scheme for below\n",
    "    colors_below = ['#fff5f0', '#fee0d2', '#fcbba1', '#fc9272', '#fb6a4a', '#ef3b2c', '#cb181d', '#99000d'][::-1]\n",
    "    cmap_below = LinearSegmentedColormap.from_list('custom_reds', colors_below)\n",
    "    \n",
    "    \n",
    "    for state in range(3):\n",
    "        # Create a new map for this season and state\n",
    "        base_map_above = np.full((1801, 3600), np.nan)\n",
    "        base_map_below = np.full((1801, 3600), np.nan)\n",
    "        \n",
    "        # For each location, find the most likely category and its probability\n",
    "        for country in countries:\n",
    "            idx_locations = np.array(index_all[country])\n",
    "            data = np.array(data_all[country])\n",
    "            for coord_idx, (lat_idx, lon_idx) in enumerate(idx_locations):\n",
    "                # Get probabilities for all states in this season\n",
    "                probs_ab = data[coord_idx, state , 0]\n",
    "                probs_bl = data[coord_idx, state , 1]\n",
    "                \n",
    "                base_map_above[lat_idx, lon_idx] = probs_ab\n",
    "                base_map_below[lat_idx, lon_idx] = probs_bl\n",
    "\n",
    "        # Create the plot\n",
    "        fig = plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        # Create projection\n",
    "        projection = ccrs.PlateCarree(central_longitude=180)\n",
    "        data_transform = ccrs.PlateCarree()\n",
    "        \n",
    "        # Above Normal p-values (left plot)\n",
    "        ax1 = plt.subplot(121, projection=projection)\n",
    "        ax1.coastlines(linewidth=0.5)\n",
    "        ax1.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "        \n",
    "        img1 = ax1.imshow(base_map_above,\n",
    "                        transform=data_transform,\n",
    "                        extent=[0, 360, -90, 90],\n",
    "                        cmap=cmap_above,\n",
    "                        vmin=0,\n",
    "                        vmax=1)\n",
    "        \n",
    "        # Add colorbar for above\n",
    "        cbar1 = plt.colorbar(img1, orientation='horizontal', pad=0.1)\n",
    "        cbar1.set_label('P-value Above Normal')\n",
    "        \n",
    "        # Add gridlines\n",
    "        gl1 = ax1.gridlines(draw_labels=True, linewidth=0.2, color='gray', alpha=0.5)\n",
    "        gl1.top_labels = False\n",
    "        gl1.right_labels = False\n",
    "        \n",
    "        # Title for left plot\n",
    "        ax1.set_title('Above Normal P-values', fontsize=12, pad=10)\n",
    "        \n",
    "        # Below Normal p-values (right plot)\n",
    "        ax2 = plt.subplot(122, projection=projection)\n",
    "        ax2.coastlines(linewidth=0.5)\n",
    "        ax2.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "        \n",
    "        img2 = ax2.imshow(base_map_below,\n",
    "                        transform=data_transform,\n",
    "                        extent=[0, 360, -90, 90],\n",
    "                        cmap=cmap_below,\n",
    "                        vmin=0,\n",
    "                        vmax=1)\n",
    "        \n",
    "        # Add colorbar for below\n",
    "        cbar2 = plt.colorbar(img2, orientation='horizontal', pad=0.1)\n",
    "        cbar2.set_label('P-value Below Normal')\n",
    "        \n",
    "        # Add gridlines\n",
    "        gl2 = ax2.gridlines(draw_labels=True, linewidth=0.2, color='gray', alpha=0.5)\n",
    "        gl2.top_labels = False\n",
    "        gl2.right_labels = False\n",
    "        \n",
    "        # Title for right plot\n",
    "        ax2.set_title('Below Normal P-values', fontsize=12, pad=10)\n",
    "        \n",
    "        filename = f'pvalue_maps_{states_lower[state]}.png'\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, \n",
    "                   dpi=300,              # Resolution\n",
    "                   bbox_inches='tight',   # Trim extra white space\n",
    "                   facecolor='white',     # White background\n",
    "                   format='png')          # File format\n",
    "        \n",
    "        # Show the plot (optional)\n",
    "        # plt.show()\n",
    "        \n",
    "        # Close the figure to free memory\n",
    "        plt.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_probability_maps(freq_all_country, points_idx_climate, output_dir=f'outcome/map/{climate_var}')\n",
    "create_dual_p_value_maps(significance_all_country, points_idx_climate, output_dir=f'outcome/map/{climate_var}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
